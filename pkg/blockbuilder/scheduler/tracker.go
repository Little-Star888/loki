package scheduler

import (
	"context"
	"sort"
	"sync"

	"github.com/go-kit/log"
	"github.com/go-kit/log/level"

	"github.com/grafana/loki/v3/pkg/blockbuilder/types"
)

type Committer interface {
	Commit(ctx context.Context, partition int32, offset int64) error
}

/*
why do we need a commit tracker?
given jobs generated by the planner can be processed parallelly and out of order, we need to keep track of the offset that have been completely consumed before committing them.
if there are gaps in what was consumed, tracker has to wait for those gaps to be consumed before committing.

Start building with the assumption that the scheduler does not lose it's state - does not restart
We can write the completion messages to kafka in a follow-up to sustain restarts.

read completion messages from the last 2h by default to recover the state and commit any missing offsets.
first thing to do before marking ready?

what about the case when there are no commits?
planner and commit tracker should start from the same offset.
to keep things simple, we can assume that consumption start from start of the previous day if there are no commits.
This should ensure all components even across restarts are aligned.
*/

type CommitTracker struct {
	mu sync.Mutex

	// invariant: sorted asc by min offset
	uncommittedOffsets map[int32][]types.Offsets
	lastCommitted      map[int32]int64

	committer            Committer
	offsetReader         OffsetReader
	fallbackOffsetMillis int64
	logger               log.Logger
}

func NewCommitTracker(committer Committer, offsetManager OffsetReader, fallbackOffsetMillis int64, logger log.Logger) *CommitTracker {
	return &CommitTracker{
		committer:            committer,
		offsetReader:         offsetManager,
		fallbackOffsetMillis: fallbackOffsetMillis,
		uncommittedOffsets:   make(map[int32][]types.Offsets),
		lastCommitted:        make(map[int32]int64),
		logger:               logger,
	}
}

func (c *CommitTracker) starting(ctx context.Context) error {
	level.Info(c.logger).Log("msg", "starting commit tracker")

	// read last committed offsets from kafka
	lag, err := c.offsetReader.GroupLag(ctx, c.fallbackOffsetMillis)
	if err != nil {
		level.Error(c.logger).Log("msg", "failed to get group lag", "err", err)
		return err
	}

	for partition, lag := range lag {
		c.lastCommitted[partition] = lag.Commit.At
	}

	// TODO: read completed-jobs topic to recover state and commit eligible offsets so planner can avoid creating jobs that were already processed.
	return nil
}

func (c *CommitTracker) ProcessCompletedJob(ctx context.Context, job *types.Job) error {
	level.Debug(c.logger).Log("msg", "processing completed job", "id", job.ID(), "partition", job.Partition(), "offsets", job.Offsets())

	c.mu.Lock()
	defer c.mu.Unlock()

	completedJobs := append(c.uncommittedOffsets[job.Partition()], job.Offsets())
	sort.Slice(completedJobs, func(i, j int) bool {
		return completedJobs[i].Min < completedJobs[j].Min
	})

	lastCommitted, err := c.getLastCommittedOffset(ctx, job.Partition())
	if err != nil {
		level.Error(c.logger).Log("msg", "failed to get last committed offset", "err", err, "partition", job.Partition())
		return err
	}

	committedUptoIdx := -1
	offsetToCommit := lastCommitted
	for idx, offset := range completedJobs {
		// already committed, skip these offsets
		if offset.Max-1 <= offsetToCommit {
			committedUptoIdx = idx
			continue
		}

		// TODO: also consider overlapping ranges?
		// might be a problem when multiple schedulers start creating jobs for a new consumer group with a skew
		// since the last committed offset is determined by their respective start time.

		// continue committing until we find a gap
		if offset.Min == offsetToCommit+1 {
			offsetToCommit = offset.Max - 1
		} else {
			break
		}

		committedUptoIdx = idx
	}

	if offsetToCommit > lastCommitted {
		if err := c.committer.Commit(ctx, job.Partition(), offsetToCommit); err != nil {
			level.Error(c.logger).Log("msg", "failed to commit offsets", "err", err, "partition", job.Partition(), "offset", offsetToCommit)
			return err
		}

		c.lastCommitted[job.Partition()] = offsetToCommit
	}

	// remove committed offsets from uncommitted offsets
	c.uncommittedOffsets[job.Partition()] = completedJobs[committedUptoIdx+1:]

	return nil
}

func (c *CommitTracker) getLastCommittedOffset(ctx context.Context, partition int32) (int64, error) {
	if offset, ok := c.lastCommitted[partition]; ok {
		return offset, nil
	}

	// this could be a new partition, fetch the last committed offset from kafka.
	offsets, err := c.offsetReader.GroupLag(ctx, c.fallbackOffsetMillis)
	if err != nil {
		return -1, err
	}

	lastCommitted := offsets[partition].Commit.At
	// update the state
	c.lastCommitted[partition] = lastCommitted
	return lastCommitted, nil
}
